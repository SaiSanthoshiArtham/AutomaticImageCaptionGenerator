{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting caption.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile caption.py\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "from pickle import load\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 34\n",
    "\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "filename = \"C:/Users/VT/Desktop/Flickr Dataset/Flickr dataset/Flickr8k.token.txt\"\n",
    "doc = load_doc(filename)\n",
    "\n",
    "\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "\n",
    "descriptions = load_descriptions(doc)\n",
    "\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()\n",
    "            desc = [word.lower() for word in desc]\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "\n",
    "\n",
    "\n",
    "clean_descriptions(descriptions)\n",
    "\n",
    "\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "\n",
    "filename = 'C:/Users/VT/Desktop/Flickr Dataset/Flickr dataset/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "\n",
    "\n",
    "\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "\n",
    "train_descriptions = load_clean_descriptions('C:/Users/VT/Desktop/Flickr Dataset/Flickr dataset/descriptions.txt', train)\n",
    "\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "    for cap in val:\n",
    "        all_train_captions.append(cap)\n",
    "\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "    nsents += 1\n",
    "    for w in sent.split(' '):\n",
    "        word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "    wordtoix[w] = ix\n",
    "    ixtoword[ix] = w\n",
    "    ix += 1\n",
    "\n",
    "\n",
    "def greedySearch(photo, model):\n",
    "    in_text = 'startseq'        \n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        photo = np.resize(photo, (1,2048))\n",
    "        yhat = model.predict([photo,sequence], verbose=0)        \n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "def get_predictions():\n",
    "    model = tf.keras.models.load_model('C:/Users/VT/Desktop/Imagecaptiongenerator/model_4.h5')\n",
    "# model.load_weights('C:/Users/Laxmi/Desktop/Image captioning/model_4.h5')\n",
    "\n",
    "    images = 'C:/Users/VT/Desktop/Flickr Dataset/Flicker8k_Dataset_Image/'\n",
    "    with open(\"C:/Users/VT/Desktop/Flickr Dataset/Flickr dataset/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
    "        encoding_test = load(encoded_pickle)\n",
    "    index = np.random.choice(1000)\n",
    "    pic = list(encoding_test.keys())[index] \n",
    "    image = encoding_test[pic].reshape((1,2048))\n",
    "    x=plt.imread(images+pic)\n",
    "    st.sidebar.image(x,use_column_width=True)  \n",
    "    caption=greedySearch(image,model)\n",
    "    st.write('## Caption Generated is: ')\n",
    "    st.write(caption)\n",
    "\n",
    "st.title('Image caption generator')\n",
    "# if st.button('Generate a Random Image'):\n",
    "\n",
    "st.sidebar.markdown('## Input Image')\n",
    "if st.button('Generate a Random Image'):\n",
    "    get_predictions()\n",
    "\n",
    "      \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
